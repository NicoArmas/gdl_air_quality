{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TorchSpatiotemporal/tsl/blob/main/examples/notebooks/a_gentle_introduction_to_tsl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xxE1lI-YVk2"
   },
   "source": [
    "# A Gentle Introduction to tsl\n",
    "---\n",
    "\n",
    "This a tutorial notebook about __tsl (Torch Spatiotemporal)__, a Python library built upon the PyTorch ecosystem\n",
    "and tailored for spatiotemporal data processing.\n",
    "\n",
    "In this notebook we are going to see what are the necessary steps from data loading to model training.\n",
    "\n",
    "## Installation\n",
    "---\n",
    "\n",
    "Let's start by the installation. If you run the notebook in colab, you can install tsl with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJwaWoSrYVk3"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/TorchSpatiotemporal/tsl.git\n",
    "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-1.10.1+cu113.html\n",
    "!pip install ./tsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ZxoqCutTYVk4"
   },
   "source": [
    "In particular, the second command is installing `torch-geometric` dependencies for the specific environment you have on colab with GPU runtime. Please refer to [PyG installation guidelines](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) for installation in other environments.\n",
    "\n",
    "We recommend to install tsl from GitHub repository at the moment, to be sure you are up-to-date with latest version.\n",
    "\n",
    "Let's check if everything is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hLAxpbejYVk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsl version  : 0.1.0\n",
      "torch version: 1.11.0+cu102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/tqdm-4.64.0-py3.9.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tsl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "tsl.logger.disabled = True\n",
    "\n",
    "print(f\"tsl version  : {tsl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "D_bK40BaYVk5"
   },
   "source": [
    "## Usage\n",
    "---\n",
    "\n",
    "tsl is more than a collection of layers. We can classify the library modules into:\n",
    "\n",
    "* __Data loading modules__\n",
    "    Manage how to store, preprocess and visualize spatio-temporal data\n",
    "* __Inference modules__\n",
    "    Methods and models exploiting the data to make inferences\n",
    "\n",
    "We will go deeper on them in next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wy4OORcYVk5"
   },
   "source": [
    "## Data loading\n",
    "---\n",
    "\n",
    "`tsl` comes with several datasets used in spatiotemporal processing literature. You can find them inside the submodule `tsl.datasets`.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "As an example, we start by using the [MetrLA](https://paperswithcode.com/sota/traffic-prediction-on-metr-la) dataset, a common benchmark for traffic forecasting. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County, aggregated in 5 minute intervals for 4 months between March 2012 to June 2012. Loading the dataset is as simple as that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BsPkG4uMYVk5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found temporal data pickle, loading...\tDONE!\n",
      "Found a valid build, loading... \tDONE!\n",
      "AQ(length=26309, n_nodes=70, n_channels=1)\n"
     ]
    }
   ],
   "source": [
    "from airquality import AirQuality as AQ\n",
    "\n",
    "dataset = AQ(data_dir='../data', is_subgraph=True, sub_size = 70, sub_start= '6.0-79.0-8002.0')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67IyIupLYVk6"
   },
   "source": [
    "We can see that data are organized a 3-dimensional array, with:\n",
    "\n",
    "* 34.272 temporal steps (1 each 5 minute for 4 months)\n",
    "* 207 nodes (the loop detectors)\n",
    "* 1 channels (detected speed)\n",
    "\n",
    "Nice! Other than storing the data of interest, the dataset comes with useful tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L74iikTiYVk6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling period: <Hour>\n",
      "Has missing values: True\n",
      "Percentage of missing values: 25.67%\n",
      "Has dataset exogenous variables: False\n",
      "Relevant attributes: dist\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampling period: {dataset.freq}\\n\"\n",
    "      f\"Has missing values: {dataset.has_mask}\\n\"\n",
    "      f\"Percentage of missing values: {(1 - dataset.mask.mean()) * 100:.2f}%\\n\"\n",
    "      f\"Has dataset exogenous variables: {dataset.has_exogenous}\\n\"\n",
    "      f\"Relevant attributes: {', '.join(dataset.attributes.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC8LfuK4YVk6"
   },
   "source": [
    "Let's look at the output. We know that the dataset has missing entries, with `dataset.mask` being a binary indicator associated with each timestep, node and channel (with ones indicating valid values).\n",
    "\n",
    "Also, the dataset has no exogenous variables, i.e., there are no time-varying features paired with the main signal.\n",
    "Instead it has a useful attribute: the distance matrix. We call *attributes*, features that are static.\n",
    "\n",
    "You can access exogenous variables and attributes by `dataset.name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_YgmDUtWYVk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            256         585         586         587         588         589  \\\n",
      "256    0.000000  252.826983  252.536928  251.740761  231.443093  223.965083   \n",
      "585  252.826983    0.000000    2.883137    5.839589   35.657821   46.024426   \n",
      "586  252.536928    2.883137    0.000000    8.633242   33.308270   43.753870   \n",
      "587  251.740761    5.839589    8.633242    0.000000   39.556224   49.651786   \n",
      "588  231.443093   35.657821   33.308270   39.556224    0.000000   10.563369   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "661  113.036321  146.810371  145.955655  146.958344  120.800169  112.433682   \n",
      "663  235.261656   32.229345   34.418405   27.038035   53.429740   60.947425   \n",
      "664  273.522002   80.547749   83.430880   75.001762  111.915276  120.591217   \n",
      "665  177.811511  114.900296  112.896597  117.673114   80.256299   69.720197   \n",
      "666  183.993799  132.404073  130.182809  135.699818   96.943323   86.435629   \n",
      "\n",
      "            590         594         595         596  ...         656  \\\n",
      "256  248.370495  496.774632  522.310010  519.189804  ...  265.753810   \n",
      "585   12.696312  463.860690  509.306663  486.330269  ...  110.107532   \n",
      "586    9.953156  460.978969  506.435374  483.447653  ...  107.224516   \n",
      "587   17.868201  469.377416  514.700738  491.869788  ...  115.705954   \n",
      "588   23.932907  434.125443  478.390610  456.825718  ...   83.243180   \n",
      "..          ...         ...         ...         ...  ...         ...   \n",
      "661  140.186569  437.103507  471.765402  460.273440  ...  155.178534   \n",
      "663   40.411322  487.546359  531.641358  510.253787  ...  135.866420   \n",
      "664   92.834601  544.379093  589.582476  566.868075  ...  190.652438   \n",
      "665  104.056518  388.907569  428.121406  412.087927  ...   87.978786   \n",
      "666  120.857709  367.598216  406.369422  390.794198  ...   85.022446   \n",
      "\n",
      "            657         658         659         660         661         663  \\\n",
      "256  255.815869  253.708031  289.893906  290.558672  113.036321  235.261656   \n",
      "585   62.080065   65.285612   85.408981   96.035640  146.810371   32.229345   \n",
      "586   59.198486   62.402607   82.770378   93.323464  145.955655   34.418405   \n",
      "587   67.721988   70.862241   91.219556  101.871648  146.958344   27.038035   \n",
      "588   39.227823   40.645567   72.712492   80.181709  120.800169   53.429740   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "661  142.826973  140.677602  176.891480  177.868842    0.000000  136.651980   \n",
      "663   89.106272   91.595886  116.374417  126.381037  136.651980    0.000000   \n",
      "664  142.607431  145.832390  162.485035  174.013261  187.904868   60.744426   \n",
      "665   85.908326   82.709894  115.794669  114.431450   68.054197  120.574817   \n",
      "666   94.797126   91.079523  120.043494  116.436775   80.511075  140.848870   \n",
      "\n",
      "            664         665         666  \n",
      "256  273.522002  177.811511  183.993799  \n",
      "585   80.547749  114.900296  132.404073  \n",
      "586   83.430880  112.896597  130.182809  \n",
      "587   75.001762  117.673114  135.699818  \n",
      "588  111.915276   80.256299   96.943323  \n",
      "..          ...         ...         ...  \n",
      "661  187.904868   68.054197   80.511075  \n",
      "663   60.744426  120.574817  140.848870  \n",
      "664    0.000000  180.831237  201.426925  \n",
      "665  180.831237    0.000000   22.070142  \n",
      "666  201.426925   22.070142    0.000000  \n",
      "\n",
      "[70 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1uMQREKYVk7"
   },
   "source": [
    "This matrix stores the pairwise distance between sensors, with `inf` denoting two non-neighboring sensors.\n",
    "\n",
    "Let's now check how the speed readings look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hxqyccSSYVk7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th>1.0-103.0-11.0</th>\n",
       "      <th>1.0-113.0-3.0</th>\n",
       "      <th>1.0-73.0-23.0</th>\n",
       "      <th>10.0-1.0-2.0</th>\n",
       "      <th>10.0-3.0-1007.0</th>\n",
       "      <th>10.0-3.0-1008.0</th>\n",
       "      <th>10.0-3.0-2004.0</th>\n",
       "      <th>10.0-5.0-1002.0</th>\n",
       "      <th>11.0-1.0-41.0</th>\n",
       "      <th>11.0-1.0-43.0</th>\n",
       "      <th>...</th>\n",
       "      <th>80.0-2.0-12.0</th>\n",
       "      <th>80.0-2.0-14.0</th>\n",
       "      <th>9.0-1.0-10.0</th>\n",
       "      <th>9.0-1.0-1123.0</th>\n",
       "      <th>9.0-11.0-124.0</th>\n",
       "      <th>9.0-3.0-1003.0</th>\n",
       "      <th>9.0-3.0-25.0</th>\n",
       "      <th>9.0-5.0-5.0</th>\n",
       "      <th>9.0-9.0-2123.0</th>\n",
       "      <th>9.0-9.0-27.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>...</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 05:00:00</th>\n",
       "      <td>10.900000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>11.440000</td>\n",
       "      <td>5.4</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.166668</td>\n",
       "      <td>116.166664</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 06:00:00</th>\n",
       "      <td>9.428533</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>5.2</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>12.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.833332</td>\n",
       "      <td>137.500000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 07:00:00</th>\n",
       "      <td>9.472011</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>141.166672</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 08:00:00</th>\n",
       "      <td>9.545652</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>12.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 09:00:00</th>\n",
       "      <td>9.561413</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>7.6</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 10:00:00</th>\n",
       "      <td>9.494837</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.2</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 11:00:00</th>\n",
       "      <td>9.305163</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 12:00:00</th>\n",
       "      <td>9.221739</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>789.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 13:00:00</th>\n",
       "      <td>8.963043</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.700001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>809.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 14:00:00</th>\n",
       "      <td>8.591033</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>17.700001</td>\n",
       "      <td>4.6</td>\n",
       "      <td>17.200001</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 695 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes               1.0-103.0-11.0 1.0-113.0-3.0 1.0-73.0-23.0 10.0-1.0-2.0  \\\n",
       "channels                      PM25          PM25          PM25         PM25   \n",
       "DateTime                                                                      \n",
       "2018-01-01 05:00:00      10.900000     19.142857     11.440000          5.4   \n",
       "2018-01-01 06:00:00       9.428533     16.000000     13.400000          5.2   \n",
       "2018-01-01 07:00:00       9.472011     15.000000     21.100000          8.0   \n",
       "2018-01-01 08:00:00       9.545652     15.000000     17.400000          7.7   \n",
       "2018-01-01 09:00:00       9.561413     13.000000      5.400000          7.6   \n",
       "2018-01-01 10:00:00       9.494837     11.000000      5.500000          6.2   \n",
       "2018-01-01 11:00:00       9.305163     11.000000     15.200000          5.5   \n",
       "2018-01-01 12:00:00       9.221739      8.000000     12.300000          5.5   \n",
       "2018-01-01 13:00:00       8.963043      5.000000     16.000000          3.9   \n",
       "2018-01-01 14:00:00       8.591033      6.000000     17.700001          4.6   \n",
       "\n",
       "nodes               10.0-3.0-1007.0 10.0-3.0-1008.0 10.0-3.0-2004.0  \\\n",
       "channels                       PM25            PM25            PM25   \n",
       "DateTime                                                              \n",
       "2018-01-01 05:00:00       13.000000            10.0             7.2   \n",
       "2018-01-01 06:00:00       12.100000            12.1             5.6   \n",
       "2018-01-01 07:00:00       13.000000            12.8             5.1   \n",
       "2018-01-01 08:00:00       14.100000            12.5             4.8   \n",
       "2018-01-01 09:00:00       15.700000             7.9             4.2   \n",
       "2018-01-01 10:00:00       18.000000             4.3             3.8   \n",
       "2018-01-01 11:00:00       19.100000             4.7             4.4   \n",
       "2018-01-01 12:00:00       18.900000             6.6             5.6   \n",
       "2018-01-01 13:00:00       18.700001             7.7             7.3   \n",
       "2018-01-01 14:00:00       17.200001             5.9             6.5   \n",
       "\n",
       "nodes               10.0-5.0-1002.0 11.0-1.0-41.0 11.0-1.0-43.0  ...  \\\n",
       "channels                       PM25          PM25          PM25  ...   \n",
       "DateTime                                                         ...   \n",
       "2018-01-01 05:00:00             5.2          10.0          12.0  ...   \n",
       "2018-01-01 06:00:00             5.2          14.0          11.0  ...   \n",
       "2018-01-01 07:00:00             5.2          11.0          11.0  ...   \n",
       "2018-01-01 08:00:00             6.1          16.0           9.0  ...   \n",
       "2018-01-01 09:00:00             6.0           8.0          10.0  ...   \n",
       "2018-01-01 10:00:00             5.4          11.0           9.0  ...   \n",
       "2018-01-01 11:00:00             6.6           9.0           8.0  ...   \n",
       "2018-01-01 12:00:00             6.5           8.0           6.0  ...   \n",
       "2018-01-01 13:00:00             6.1           8.0           7.0  ...   \n",
       "2018-01-01 14:00:00             4.7           8.0           6.0  ...   \n",
       "\n",
       "nodes               80.0-2.0-12.0 80.0-2.0-14.0 9.0-1.0-10.0 9.0-1.0-1123.0  \\\n",
       "channels                     PM25          PM25         PM25           PM25   \n",
       "DateTime                                                                      \n",
       "2018-01-01 05:00:00     53.166668    116.166664         11.0            9.0   \n",
       "2018-01-01 06:00:00     61.833332    137.500000          8.0            9.0   \n",
       "2018-01-01 07:00:00     71.500000    141.166672          9.0            5.0   \n",
       "2018-01-01 08:00:00    344.000000    622.000000          8.0            3.0   \n",
       "2018-01-01 09:00:00    292.000000    744.000000          5.0            4.0   \n",
       "2018-01-01 10:00:00    278.000000    678.000000          5.0            4.0   \n",
       "2018-01-01 11:00:00    296.000000    688.000000          4.0            5.0   \n",
       "2018-01-01 12:00:00    286.000000    789.000000          4.0            4.0   \n",
       "2018-01-01 13:00:00    297.000000    809.000000          5.0            4.0   \n",
       "2018-01-01 14:00:00    385.000000    729.000000          3.0            3.0   \n",
       "\n",
       "nodes               9.0-11.0-124.0 9.0-3.0-1003.0 9.0-3.0-25.0 9.0-5.0-5.0  \\\n",
       "channels                      PM25           PM25         PM25        PM25   \n",
       "DateTime                                                                     \n",
       "2018-01-01 05:00:00            7.0            6.0          8.0         7.0   \n",
       "2018-01-01 06:00:00            7.0            6.0         10.0         6.0   \n",
       "2018-01-01 07:00:00            6.0            8.0          7.0         5.0   \n",
       "2018-01-01 08:00:00            3.0            7.0          3.0         1.0   \n",
       "2018-01-01 09:00:00            3.0            5.0          3.0         4.0   \n",
       "2018-01-01 10:00:00            3.0            5.0          3.0         6.0   \n",
       "2018-01-01 11:00:00            5.0            0.0          2.0         1.0   \n",
       "2018-01-01 12:00:00            5.0            1.0          2.0         2.0   \n",
       "2018-01-01 13:00:00            5.0            5.0          2.0         1.0   \n",
       "2018-01-01 14:00:00            4.0            6.0          3.0         1.0   \n",
       "\n",
       "nodes               9.0-9.0-2123.0 9.0-9.0-27.0  \n",
       "channels                      PM25         PM25  \n",
       "DateTime                                         \n",
       "2018-01-01 05:00:00            0.0          3.0  \n",
       "2018-01-01 06:00:00            1.0          3.0  \n",
       "2018-01-01 07:00:00            4.0          3.0  \n",
       "2018-01-01 08:00:00            3.0          1.0  \n",
       "2018-01-01 09:00:00            1.0          0.0  \n",
       "2018-01-01 10:00:00            4.0          3.0  \n",
       "2018-01-01 11:00:00            3.0          4.0  \n",
       "2018-01-01 12:00:00            1.0          2.0  \n",
       "2018-01-01 13:00:00            5.0          2.0  \n",
       "2018-01-01 14:00:00            8.0          1.0  \n",
       "\n",
       "[10 rows x 695 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataframe().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN0pLP1IYVk8"
   },
   "source": [
    "### Get connectivity\n",
    "\n",
    "Besides the time series, to properly use graph-based models, we need to __connect__ nodes somehow.\n",
    "\n",
    "With the method `dataset.get_similarity()` we can retrieve nodes' similarities computed with different methods. The available similarity methods for a dataset can be found at `dataset.similarity_options`, while the default one is at `dataset.similarity_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NRZHPBhmYVk8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default similarity: distance\n",
      "Available similarity options: {'distance'}\n",
      "\n",
      "[[1.         0.9564619  0.99390571 0.52242575 0.51529322 0.50860576\n",
      "  0.50183616 0.53325834 0.59217674 0.5940329 ]\n",
      " [0.9564619  1.         0.97783375 0.52818064 0.5114032  0.50527559\n",
      "  0.49656053 0.54387295 0.58860374 0.58955044]\n",
      " [0.99390571 0.97783375 1.         0.49541105 0.48521184 0.4787652\n",
      "  0.47144431 0.50778061 0.56204142 0.56361064]\n",
      " [0.52242575 0.52818064 0.49541105 1.         0.99788113 0.99779702\n",
      "  0.99644182 0.99929262 0.99250402 0.99193799]\n",
      " [0.51529322 0.5114032  0.48521184 0.99788113 1.         0.99993399\n",
      "  0.99966454 0.99489148 0.99165567 0.99135145]\n",
      " [0.50860576 0.50527559 0.4787652  0.99779702 0.99993399 1.\n",
      "  0.99982768 0.99464621 0.99019887 0.98985155]\n",
      " [0.50183616 0.49656053 0.47144431 0.99644182 0.99966454 0.99982768\n",
      "  1.         0.99260447 0.98821162 0.98790065]\n",
      " [0.53325834 0.54387295 0.50778061 0.99929262 0.99489148 0.99464621\n",
      "  0.99260447 1.         0.99271759 0.9920618 ]\n",
      " [0.59217674 0.58860374 0.56204142 0.99250402 0.99165567 0.99019887\n",
      "  0.98821162 0.99271759 1.         0.99998409]\n",
      " [0.5940329  0.58955044 0.56361064 0.99193799 0.99135145 0.98985155\n",
      "  0.98790065 0.9920618  0.99998409 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default similarity: {dataset.similarity_score}\\n\"\n",
    "      f\"Available similarity options: {dataset.similarity_options}\\n\")\n",
    "\n",
    "sim = dataset.get_similarity(\"distance\")  # same as dataset.get_similarity()\n",
    "print(sim[:10, :10])  # just check first 10 nodes for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCcgblQAYVk8"
   },
   "source": [
    "With this method, we compute weight $w_t^{i,j}$ of the edge connecting $i$-th and $j$-th node as\n",
    "$$\n",
    "w^{i,j} = \\left\\{\\begin{array}{cl}\n",
    "     \\exp \\left(-\\frac{\\operatorname{dist}\\left(i, j\\right)^{2}}{\\gamma}\\right) & \\operatorname{dist}\\left(i, j\\right) \\leq \\delta  \\\\\n",
    "     0 & \\text{otherwise}\n",
    "\\end{array}\\right. ,\n",
    "$$\n",
    "where $\\operatorname{dist}\\left(i, j\\right)$ is the distance between $i$-th and $j$-th node, $\\gamma$ controls the width of the kernel and $\\delta$ is a threshold. Notice that in this case the similarity matrix is not symmetric, since the original preprocessed distance matrix is not symmetric too.\n",
    "\n",
    "So far so good, now we can build an adjacency matrix out ouf the computed similarity.\n",
    "\n",
    "The method `dataset.get_connectivity()`, wraps the `dataset.get_similarity()` module, provides useful preprocessing options, and, eventually, returns a sparse (weighted) adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "goTBHPNGYVk8"
   },
   "outputs": [],
   "source": [
    "adj = dataset.get_connectivity(threshold=0.1,\n",
    "                               include_self=False,\n",
    "                               normalize_axis=1,\n",
    "                               layout=\"edge_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsPSdKmRYVk9"
   },
   "source": [
    "With this function call, under the hood we:\n",
    "\n",
    "1. compute the similarity matrix as before\n",
    "1. set to $0$ values below $0.1$ (`threshold=0.1`)\n",
    "1. remove self loops (`include_self=False`)\n",
    "1. normalize edge weights by the in degree of nodes (`normalize_axis=1`)\n",
    "1. request the sparse COO layout of PyG (`layout=\"edge_index\"`)\n",
    "\n",
    "The connectivity matrix with `edge_index` layout is provided in COO format, adopting the convention and notation used in PyTorch Geometric. The returned connectivity is a tuple (`edge_index`, `edge_weight`), where `edge_index` lists all edges as pairs of source-target nodes (dimensions `[2, E]`) and `edge_weight` (dimension `[E]`) stores the corresponding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova = 70 ** 2\n",
    "prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jD8S74swYVk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2316)\n",
      "[0.00549751 0.0061028  0.00430149 ... 0.02189975 0.01458481 0.03844911]\n"
     ]
    }
   ],
   "source": [
    "edge_index, edge_weight = adj\n",
    "\n",
    "print(edge_index.shape)\n",
    "print(edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2584"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova - edge_index.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287010,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287010"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(edge_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaGwFbs4YVk9"
   },
   "source": [
    "The dense layout corresponds to the weighted adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$. The module `tsl.ops.connectivity` contains useful operations for connectivities, including methods to change layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gugmaNIpYVk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 437)\n"
     ]
    }
   ],
   "source": [
    "from tsl.ops.connectivity import edge_index_to_adj\n",
    "\n",
    "dense = edge_index_to_adj(edge_index, edge_weight)\n",
    "print(dense.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZB1SiBYYVk-"
   },
   "source": [
    "## Data processing\n",
    "---\n",
    "\n",
    "In this section, we will see how to transfer data from a dataset to an inference model (e.g., a spatiotemporal graph neural network).\n",
    "\n",
    "### The SpatioTemporalDataset\n",
    "\n",
    "The first class that comes in help is `tsl.data.SpatioTemporalDataset`. This class is a subclass of `torch.utils.data.Dataset` can be considered as wrapper of a `tsl` dataset providing the interface for further processing.\n",
    "\n",
    "In particular, a `SpatioTemporalDataset` object can be used to achieve the following:\n",
    "* perform the transformations required to feed the data to a model (e.g., casting to `torch.tensor`, handling different `shapes`)\n",
    "* handling temporal slicing and windowing for training (e.g., split data in $\\left( \\text{window}, \\text{horizon} \\right)$ samples)\n",
    "* defining the layout of inputs and targets (e.g., how node attributes and exogenous variables are arranged)\n",
    "* preprocess data before creating a batch\n",
    "\n",
    "Let's see how to go from a `Dataset` to a `SpatioTemporalDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s2VPMgQnYVk-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataset(n_samples=8737, n_nodes=437, n_channels=1)\n"
     ]
    }
   ],
   "source": [
    "from tsl.data import SpatioTemporalDataset\n",
    "\n",
    "torch_dataset = SpatioTemporalDataset(*dataset.numpy(return_idx=True),\n",
    "                                      connectivity=adj,\n",
    "                                      mask=dataset.mask,\n",
    "                                      horizon=12,\n",
    "                                      window=12)\n",
    "print(torch_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ap77CYPYVk-"
   },
   "source": [
    "As you can see, the number of samples is not the same as the number of steps we have in the dataset. Indeed, we divided the time series with a sliding window of 12 steps for the input (`window=12`), with a corresponding horizon of 12 steps (`horizon=12`). Thus, a sample spans for a total of $24$ steps. But let's look in details to the layout of a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_G7-8HMcYVk_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(input:{x=[12, 437, 1], edge_index=[2, 5398], edge_weight=[5398]}, target:{y=[12, 437, 1]}, has_mask=True)\n"
     ]
    }
   ],
   "source": [
    "sample = torch_dataset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXSGWvK5YVk_"
   },
   "source": [
    "A sample has 5 main attributes:\n",
    "\n",
    "* `sample.input` is a mapping of data to be forwarded as input to the model.\n",
    "* `sample.target` is a mapping of data to be forwarded as target for the loss function of the model.\n",
    "* `sample.mask` store the `mask`, if any. It is useful for computing the loss only on valid data.\n",
    "* `sample.transform` is a mapping containing as value a transformation function (e.g., scaling, detrending) and as key the name of the tensor to be transformed.\n",
    "* `sample.pattern` stores the pattern, i.e., a more informative shape representation, of each tensor in `sample`.\n",
    "\n",
    "Let's check more in details how each of these attributes is composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tODoWS_YVk_"
   },
   "source": [
    "#### Input and Target\n",
    "\n",
    "A sample is a `tsl.data.Data` object which stores all that is needed to support inference.\n",
    "Both `input` and `target` are `tsl.data.DataView` of this storage.\n",
    "This means that they have the same methods, but contain different subsets keys.\n",
    "As a results, you cannot store two tensors using the key in `input` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TbB9HvSoYVk_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[138.0000],\n",
       "          [ 89.0000],\n",
       "          [105.0000],\n",
       "          ...,\n",
       "          [ 61.7920],\n",
       "          [ 75.6980],\n",
       "          [ 77.2573]],\n",
       " \n",
       "         [[124.0000],\n",
       "          [ 85.0000],\n",
       "          [121.0000],\n",
       "          ...,\n",
       "          [ 59.1114],\n",
       "          [ 70.3417],\n",
       "          [ 72.0449]],\n",
       " \n",
       "         [[127.0000],\n",
       "          [ 88.0000],\n",
       "          [130.0000],\n",
       "          ...,\n",
       "          [ 58.2766],\n",
       "          [ 69.5855],\n",
       "          [ 71.1539]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[147.0000],\n",
       "          [133.0000],\n",
       "          [148.0000],\n",
       "          ...,\n",
       "          [ 52.9467],\n",
       "          [ 64.2753],\n",
       "          [ 63.6968]],\n",
       " \n",
       "         [[188.0000],\n",
       "          [ 29.0000],\n",
       "          [167.0000],\n",
       "          ...,\n",
       "          [ 53.2139],\n",
       "          [ 63.2178],\n",
       "          [ 63.1341]],\n",
       " \n",
       "         [[212.0000],\n",
       "          [ 33.3333],\n",
       "          [178.0000],\n",
       "          ...,\n",
       "          [ 55.3667],\n",
       "          [ 64.9315],\n",
       "          [ 65.9394]]]),\n",
       " 'edge_index': tensor([[  0,   0,   0,  ..., 434, 435, 436],\n",
       "         [  1,   2,   3,  ..., 433, 436, 435]]),\n",
       " 'edge_weight': tensor([0.0672, 0.0428, 0.0225,  ..., 0.3462, 1.0000, 1.0000])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.input.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HdorD4uLYVk_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': tensor([[[229.0000],\n",
       "          [111.0000],\n",
       "          [179.0000],\n",
       "          ...,\n",
       "          [ 55.8811],\n",
       "          [ 68.1196],\n",
       "          [ 66.7615]],\n",
       " \n",
       "         [[240.0000],\n",
       "          [172.0000],\n",
       "          [184.0000],\n",
       "          ...,\n",
       "          [ 53.8635],\n",
       "          [ 67.5794],\n",
       "          [ 67.3180]],\n",
       " \n",
       "         [[240.0000],\n",
       "          [173.0000],\n",
       "          [188.0000],\n",
       "          ...,\n",
       "          [ 52.7024],\n",
       "          [ 63.6870],\n",
       "          [ 63.7933]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[119.0000],\n",
       "          [ 51.0000],\n",
       "          [ 50.0000],\n",
       "          ...,\n",
       "          [ 61.6110],\n",
       "          [ 73.4010],\n",
       "          [ 70.2052]],\n",
       " \n",
       "         [[ 48.0000],\n",
       "          [ 33.0000],\n",
       "          [ 36.0000],\n",
       "          ...,\n",
       "          [ 67.3310],\n",
       "          [ 78.3555],\n",
       "          [ 77.4719]],\n",
       " \n",
       "         [[ 37.0000],\n",
       "          [ 31.0000],\n",
       "          [ 34.0000],\n",
       "          ...,\n",
       "          [ 63.4985],\n",
       "          [ 76.7035],\n",
       "          [ 82.3329]]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.target.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNq8M-yaYVk_"
   },
   "source": [
    "#### Mask and Transform\n",
    "\n",
    "`mask` and `transform` are just symbolic links to the corresponding object inside the storage. They also expose properties `has_mask` and `has_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rAlbTjo7YVlA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "if sample.has_mask:\n",
    "    print(sample.mask)\n",
    "else:\n",
    "    print(\"Sample has no mask.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zW6tx42LYVlA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample has no transform functions.\n"
     ]
    }
   ],
   "source": [
    "if sample.has_transform:\n",
    "    print(sample.transform)\n",
    "else:\n",
    "    print(\"Sample has no transform functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR-LrjOwYVlA"
   },
   "source": [
    "#### Pattern\n",
    "\n",
    "The `pattern` mapping can be useful to glimpse on how data are arranged.\n",
    "The convention we use is the following:\n",
    "\n",
    "* \"b\" stands for \"batch size\"\n",
    "* \"c\" stands for \"number of channels\" (per node)\n",
    "* \"e\" stands for \"number edges\"\n",
    "* \"n\" stands for \"number of nodes\"\n",
    "* \"s\" stands for \"number of time steps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4KeKgewEYVlA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 's n c',\n",
       " 'edge_index': '2 e',\n",
       " 'edge_weight': 'e',\n",
       " 'mask': 's n c',\n",
       " 'y': 's n c'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j-pA8lCYVlA"
   },
   "source": [
    "### The SpatioTemporalDataModule\n",
    "\n",
    "Usually, before running an experiment there are two quite common preprocessing steps:\n",
    "\n",
    "* splitting the dataset into training/validation/test sets\n",
    "* data preprocessing (scaling/normalizing data, detrending)\n",
    "\n",
    "In `tsl`, these operations are carried out in the `tsl.data.SpatioTemporalDataModule`, which is based on `pytorch-lightning`'s data modules.\n",
    "\n",
    "Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3aGnkoCgYVlA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataModule(train_len=None, val_len=None, test_len=None, scalers=[data], batch_size=64)\n"
     ]
    }
   ],
   "source": [
    "from tsl.data import SpatioTemporalDataModule\n",
    "from tsl.data.preprocessing import StandardScaler\n",
    "\n",
    "scalers = {'data': StandardScaler(axis=(0, 1))}\n",
    "\n",
    "splitter = dataset.get_splitter(val_len=0.1, test_len=0.2)\n",
    "\n",
    "dm = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    splitter=splitter,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229wC1LaYVlB"
   },
   "source": [
    " Eventually one could extend the base datamodule to add further processing in case it is needed.\n",
    "\n",
    "At this point, the `DataModule` object has not actually performed any processing yet (lazy approach).\n",
    "\n",
    "We can execute the preprocessing routines by calling `setup` method.\n",
    "\n",
    "Note that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tMpX1zbfYVlB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataModule(train_len=5141, val_len=576, test_len=2884, scalers=[data], batch_size=64)\n"
     ]
    }
   ],
   "source": [
    "dm.setup()\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXQy8qMDYVlB"
   },
   "source": [
    "After `setup` has been called, the datamodule carries the following operations:\n",
    "\n",
    "1. Carries out the dataset splitting into training/validation/test sets according to the `splitter` argument.\n",
    "1. Fits all the `Scalers` on the training data in `dataset` corresponding to the scalers' keys.\n",
    "\n",
    "#### Scalers\n",
    "\n",
    "The `tsl.data.preprocessing` package offers several of the most common data normalization techniques under the `tsl.data.preprocessing.Scaler` interface.\n",
    "They adopt an API similar to `scikit-learn`'s scalers, with `fit`/`transform`/`fit_transform`/`inverse_transform` methods. Check the documentation for more details about this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d60HdqXCYVlB"
   },
   "source": [
    "## Building a Model\n",
    "---\n",
    "\n",
    "In this section, we will see how to build a very simple Graph Neural Network for Spatiotemporal data.\n",
    "All the neural network code inside `tsl` is under the `tsl.nn` module.\n",
    "\n",
    "\n",
    "### The NN module\n",
    "\n",
    "The `tsl.nn` module is organized as follows:\n",
    "\n",
    "```\n",
    "tsl\n",
    "└── nn\n",
    "    ├── base\n",
    "    ├── blocks\n",
    "    ├── layers\n",
    "    ├── models\n",
    "    ├── metrics\n",
    "    ├── ops\n",
    "    └── utils\n",
    "```\n",
    "\n",
    "The 3 most important submodules in it are `layers`, `blocks`, and `models`, ordered by increasing level of abstraction.\n",
    "\n",
    "#### Layers\n",
    "\n",
    "A _layer_ is a basic building block for our neural networks. In simple words, a layer takes an input, performs one (or few) operations, and return a transformation of the input. Examples of layers are `DiffConv`, which implements [diffusion convolution](https://arxiv.org/abs/1707.01926), or `LayerNorm`.\n",
    "\n",
    "#### Blocks\n",
    "\n",
    "_blocks_ perform more complex transformations or combine several operations. We divide blocks into _encoders_, if they provide a representation of the input in a new space, and _decoders_, if they produce a meaningful output from a representation.\n",
    "\n",
    "#### Models\n",
    "\n",
    "We wrap a series of operations, represented by blocks and/or layers, in a _model_. A model is meant to takes as input a batch `SpatioTemporalDataset`'s items and return the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQiMvBjGYVlC"
   },
   "source": [
    "Let's create a very simple model with a RNN encoder and a nonlinear GCN readout.\n",
    "To do so, we import `RNN` from the encoders and `GCNDecoder` from the decoders in the `tsl.nn.blocks` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ORL_KKbuYVlC"
   },
   "outputs": [],
   "source": [
    "from tsl.nn.blocks.encoders import RNN\n",
    "from tsl.nn.blocks.decoders import GCNDecoder\n",
    "\n",
    "\n",
    "class TimeThenSpaceModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 rnn_layers,\n",
    "                 gcn_layers,\n",
    "                 horizon):\n",
    "        super(TimeThenSpaceModel, self).__init__()\n",
    "\n",
    "        self.input_encoder = torch.nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.encoder = RNN(input_size=hidden_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           n_layers=rnn_layers)\n",
    "\n",
    "        self.decoder = GCNDecoder(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=input_size,\n",
    "            horizon=horizon,\n",
    "            n_layers=gcn_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        # x: [batches steps nodes channels]\n",
    "        x = self.input_encoder(x)\n",
    "\n",
    "        x = self.encoder(x, return_last_state=True)\n",
    "\n",
    "        return self.decoder(x, edge_index, edge_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fu_1-ojYVlC"
   },
   "source": [
    "Fine, we have a model and we have data, now let's train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3uABycjYVlC"
   },
   "source": [
    "## Setting up training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxvEqv95YVlC"
   },
   "source": [
    "### The Predictor\n",
    "\n",
    "In `tsl`, inference engines are implemented as a [`LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.LightningModule.html#pytorch_lightning.core.LightningModule). `tsl.predictors.Predictor` is a base class that can be extended to build more complex forecasting approaches.\n",
    "These modules are meant to wrap deep models in order to ease training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GULyga_lYVlD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/pytorch_lightning-1.6.0-py3.9.egg/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'loss_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_fn'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from tsl.nn.metrics.metrics import MaskedMAE, MaskedMAPE\n",
    "from tsl.predictors import Predictor\n",
    "\n",
    "loss_fn = MaskedMAE(compute_on_step=True)\n",
    "\n",
    "metrics = {'mae': MaskedMAE(compute_on_step=False),\n",
    "           'mape': MaskedMAPE(compute_on_step=False),\n",
    "           'mae_at_15': MaskedMAE(compute_on_step=False, at=2),  # `2` indicated the third time step,\n",
    "                                                                 # which correspond to 15 minutes ahead\n",
    "           'mae_at_30': MaskedMAE(compute_on_step=False, at=5),\n",
    "           'mae_at_60': MaskedMAE(compute_on_step=False, at=11), }\n",
    "\n",
    "model_kwargs = {\n",
    "    'input_size': dm.n_channels,  # 1 channel\n",
    "    'horizon': dm.horizon,  # 12, the number of steps ahead to forecast\n",
    "    'hidden_size': 32,\n",
    "    'rnn_layers': 1,\n",
    "    'gcn_layers': 2\n",
    "}\n",
    "\n",
    "# setup predictor\n",
    "predictor = Predictor(\n",
    "    model_class=TimeThenSpaceModel,\n",
    "    model_kwargs=model_kwargs,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    optim_kwargs={'lr': 0.001},\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-34Ntg3lYVlD"
   },
   "source": [
    "Now let's finalize the last details. We make use of [TensorBoard](https://www.tensorflow.org/tensorboard/) to log and visualize metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a3zdVgNRYVlD"
   },
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "# logger = TensorBoardLogger(save_dir=\"logs\", name=\"tsl_intro\", version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger(save_dir='prova' , name='savedata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4IU2vFKYVlD"
   },
   "outputs": [],
   "source": [
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1b57xoTYVlD"
   },
   "source": [
    "We let `pytorch_lightning.Trainer` handle the dirty work for us. We can directly pass the datamodule to the trainer for fitting.\n",
    "\n",
    "If this is the case, the trainer will call the `setup` method, and then load train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WSJLVCaFYVlD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/pytorch_lightning-1.6.0-py3.9.egg/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /home/lorenzo/PycharmProjects/gdl_air_quality/code/utility notebooks/logs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | loss_fn       | MaskedMAE          | 0     \n",
      "1 | train_metrics | MetricCollection   | 0     \n",
      "2 | val_metrics   | MetricCollection   | 0     \n",
      "3 | test_metrics  | MetricCollection   | 0     \n",
      "4 | model         | TimeThenSpaceModel | 12.0 K\n",
      "-----------------------------------------------------\n",
      "12.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 K    Total params\n",
      "0.048     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/pytorch_lightning-1.6.0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/pytorch_lightning-1.6.0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 89/89 [10:27<00:00,  7.05s/it, loss=26.5, v_num=1, val_mae=25.10, val_mae_at_15=18.90, val_mae_at_30=25.80, val_mae_at_60=33.30, val_mape=0.620, train_mae=26.30, train_mae_at_15=20.60, train_mae_at_30=27.30, train_mae_at_60=33.40, train_mape=0.616]   \n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='logs',\n",
    "    save_top_k=1,\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3,\n",
    "                     logger=logger,\n",
    "                     gpus=1 if torch.cuda.is_available() else None,\n",
    "                     limit_train_batches=100,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "\n",
    "trainer.fit(predictor, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bAHFLAEYVlE"
   },
   "source": [
    "## Testing\n",
    "---\n",
    "\n",
    "\n",
    "Now let's see how the trained model behaves on new unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aV-v6EfCYVlE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/anaconda3/envs/GDL/lib/python3.9/site-packages/pytorch_lightning-1.6.0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 46/46 [00:46<00:00,  1.01s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           24.076353073120117\n",
      "        test_mae            24.635517120361328\n",
      "     test_mae_at_15         19.228456497192383\n",
      "     test_mae_at_30         25.976608276367188\n",
      "     test_mae_at_60          31.25102424621582\n",
      "        test_mape           0.6429881453514099\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "predictor.load_model(checkpoint_callback.best_model_path)\n",
    "predictor.freeze()\n",
    "\n",
    "performance = trainer.test(predictor, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRd_FMpyYVlE"
   },
   "source": [
    "Cool! We succeeded in creating first a simple, yet effective, SpatioTemporal model!\n",
    "\n",
    "We are now __tsl ninjas__. We learned how to:\n",
    "\n",
    "* Load benchmark datasets\n",
    "* Organize data for processing\n",
    "* Preprocess the data\n",
    "* Build a Spatiotemporal GNN\n",
    "* Train and evaluate the model\n",
    "\n",
    "We hope you enjoyed this introduction to `tsl`, do not hesitate to contact us if you have any question or problem while using it.\n",
    "\n",
    "The tsl team.\n",
    "\n",
    "🧡"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "a_gentle_introduction_to_tsl.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
